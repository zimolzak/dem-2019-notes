Science of Diagnostic Error (Nov 10)
=========


Laura Zwaan
--------

Definitions: many. Four major. Graber 2005, Singh 2014, Schiff 2005,
NAM 2015. Sez NAM def includes the communication and is nonspecific.
Many approaches[^graber2012] including autopsy, survey, standard pts
(time consuming), second review, dx testing audits, malpractice
claims, case reviews, voluntary reports, experimental studies,
qualitative research. They differ in whether they can study
incidences, and study etiology. (Standardized pts and voluntary
reports can do both, notably.) Only 0.5% of incident reports were
diagnostic error reports, though.

In detail, contrast chart reviews and experimental studies. Chart has
high external, low internal validity. Experimental is opposite.
Example of retro chart review: 21/100 Netherlands hospitals
participated.

Type  EV     IV
----  -----  -----
Chart high   low
Exper low    high

Tips for record review studies: inter-rater reliability was low. More
people didn't help the IRR. Because dx errors are rare, do power
calculations. If the record review questionnaire is too long or
unclear, or if incomplete training of reviewers, those are all
pitfalls. Important to predefine hypotheses and exclusion criteria (so
describe coincidental findings as exploratory.

Hindsight bias is hard to avoid, and it comes up when you look *only*
at the error cases[^zwaan2017]. Experimental study that replicated how
reviewers look at cases. Constructed so the dx process described was
the same, but the endings are different. People rate the process as
bad when the ending is bad. Also they don't agree hardly at all on the
cause (like representativeness, premature closure, etc.) of the error.

Availability bias study (recent or memorable)[^Schmidt2014]. Go read
these Wikipedia articles about Q fever or Legionnaire's disease. Then
later in the day sneak up and have them read a case. 55% correct if
exposed to wiki vs. 70% if not. Argued: this shows the bias exists,
but not how often it happens (or how often it helps for that matter).

Eye tracking. Kind of underused method. Also it's low intervention, doesn't
bother the subject. Eyelink1000 device. Chest xrays. Intervention is with the
patient description. So like have "cough" versus "fall" attached to a
clavicle fx xray. If you didn't see it in 2 seconds, you generally
didn't see it[^zwaanprep].

Further advice: not too many experimental conditions, beware
confounding, make the setup very standardized like written
instructions so the experimenter doesn't give subtle hints,
preregister the study (hypotheses and exclusion), publish negative
results if you can.

[^zwaanprep]: Zwaan *et al.*, manuscript in prep.

[^graber2012]: Graber, 2012, *BMJ QS.*

[^zwaan2017]: Zwaan. Is bias in the Eye of the beholder? *BMJ Q S*
2017, 26:104.

[^Schmidt2014]: Schmidt *et al.* 2014




Hardeep Singh
--------

Conceptual models all wrong, some are more useful. What's been useful
to us? **Missed opportunities** [^singh-jc]. Sociotechnical[^socio]:
need to account for all 8 dimensions: content, hwsw, ui, org policy,
workflow and comm, personnel, meas and mon, external rules and regs. A
lot of time they forget rules/reg. Third, Safer Dx framework breaks
down the dx proc into 5 dimensions: encounter, test perf and interp,
consult, folow up and track, and finally patient at the
center[^safer]. NAM has a conceptual model too of course, the blue
loop and red dot figure.

missed harm result
------ ---- ------
+      +
+      -
-      +
-      -

Walk thru types of data/methods. Mark Graber in *AJM* wrote about
overconf. That inspired Hardeep's study of Clinician cognition[^confid].
Difficult vs. easy vignetttes. Hard should go with less confidence,
and asking for more help. But it didn't bear out. Didn't tell them
there was such a thing as easy/hard. Not a prevalence study (reporter
made a big deal of only 60% of the easy cases were answered
correctly). 3 experts were the source of the easy/hard rating.

Opportunistic data[^gandhi][^serious][^root]. Advantages.
Disadvantages: hard to go from Root Cause Analy recs. Paper "problem
with RCA" in BMJQS. A lot of them say "do more training, do more
policy." Disadvantage: *can't generalize to frequency or population
level.* Malpractice claims don't correlate great with error. Because
not all claims are based on error.

Large (claims and/or EMR) datasets[^hyper][^stroke-ed]. Gotta validate
thru some other mechanism or else it's just hypoth generating. "Data
suggests...." Need to f/u with record review study.

Info from clinicians[^olafor]. They are forthcoming and will say they
made a dx error[^dx-peds].

Record reviews in the *non-selective* fashion[^bergl]. Consecutive
chart reviews give better idea of frequency. But it's a lot of work.
Can also do in *high risk cohort* like cancer[^endosc][^lung]. Good
agreement $\kappa = 0.69$. Missed opportunity in like 30%. Same in
peds cancer.[^bhise]

How do you review records in the first place? We have Revised Safer Dx
Instrument[^rsdi]. Another instrument Safer Dx Process Breakdown
Supplement helps you analyze the record further: what type of problem
was it?

Emerging methods. Also prospective surveillance. See Murphy's papers
in *Chest,* and mammo, and *BMJQS* "Electronic health
record-based...." Return visit triggers have low ish PPV, but these
have better.

Bench to bookshelf to bedside. Make your work visible. Prepare first
and talk to them. You and Twitter are not the only way to get it out
there. Public and policymakers have to know about your stuff.
Secondly, have clinical/operational partners. **Write conclusions and
abstract with these organizations in mind:** WHO, NAM, CDC, HHS, AMA,
AHRQ. **Who is the audience apart from other researchers?** Then get
the paper out to practicing docs, if you think the audience is
practicing docs. Incentivize the researcher to make work more
relevant, so like if WHO or NQF is impacted and/or cites your work,
etc. Furthermore, people will ask you for resources, so make one, with
hyperlinks[^compendium].

[^compendium]: A Compendium of Best Practices for IMproving Test result follow-up.

[^rsdi]: Singh. Recommendations for using the Revised.... *Diagnosis* 2019.

[^bhise]: Bhise. An electronic trigger based on care escalation. BMJQS 2018.

[^lung]: Characteristics and predictors. J clin onc.

[^endosc]: Singh. Missed opportunities to initiate endoscopic. Am J GAstro 2009

[^bergl]: Bergl PA. Frequency, risk factors, causes, and consequences.

[^olafor]: Olafor. Using voluntary reports. BMJ Emerg Med Journal. 33(4)

[^dx-peds]: Singh. Errors of diagnosis in pediatric.

[^hyper]: Alore EA. Diagnosis and management of primary hyperparathyroidism.
JAMA internal med.

[^stroke-ed]: Newman-Toker. Missed diagnosis of stroke.

[^gandhi]: Gandhi T "Missed and Delayed" Annals IM

[^serious]: Newman-Toker. "Serious Misdiagnosis-related harms"

[^root]: Giardina. "Root Cause Analysis reports Help Identify Common factors in
delayed diagnosis." 

[^confid]: Meyer *et al.* Physicians' Diagnostic Accuracy, Confidence,
and Resource Requests: A Vignette Study. *JAMA intern med* 2013

[^safer]: Singh Sittig. BMJ Q S 2015.

[^socio]: Sittig, Singh. *Qual Safe Health Care* 2010

[^singh-jc]: Singh, *Jt comm j qual patient saf* 2015.




Rob El Kareh: Large datasets
--------

Existing databases. NSQIP, SEER are clinical. HCUP, Medicare are
claims. Then there are some linked ones: Seer-Medicare,
OptumLabs[^exist]. Drawbacks: nonfatal things may not be in there,
especially subjective pt outcomes.

What types of study: SPADE[^spade]. First visit with sx and benign dx,
followed by 2nd visit with dangerous dx. Look forward and look
back. There is a simpler look-forward, so like if there's a high
calcium, what percent got a PTH test. Further, of those with
hyperpara, what percent got treatment surgery[^hyper]. Same: of those
with high Cr, who got repeat test, made a dx of CKD, and/or got renal
consult[^creat]. Assess dx proc[^breast-dx]. Find those with breast
CA, and look back to see what was the timing of the prior testing.
"Natural history of diagnosis" is not known for a lot of scenarios. Is
3 weeks good, or is 6 weeks? What does "standard of care" really mean,
and how do we find positive and negative outliers.

[^breast-dx]: Using administrative data to estimate time to breast cancer
diagnosis. Eur J Cancer Care.

[^creat]: Kaiser Permanente Creatinine Safety Program. AJM.

[^spade]: Liberman. Symptom Disease Pair Analysis of Diagnostic error.
BMJQS. 2018.

[^exist]: Murphy. Working with Existing Dabases. *Clin Colon Rectal Surg*
2013.



Traber Giardina
--------

