Diagnostic Error in Medicine Conference Notes
========

November 10 - 13, 2019. Washington, D.C.

Notes by Andrew Zimolzak, MD, MMSc




Science of Diagnostic Error Course (Nov 10)
========

Laura Zwaan
--------

Definitions: many. Four major. Graber 2005, Singh 2014, Schiff 2005,
NAM 2015. Sez NAM def includes the communication and is nonspecific.
Many approaches[^graber2012] including autopsy, survey, standard pts
(time consuming), second review, dx testing audits, malpractice
claims, case reviews, voluntary reports, experimental studies,
qualitative research. They differ in whether they can study
incidences, and study etiology. (Standardized pts and voluntary
reports can do both, notably.) Only 0.5% of incident reports were
diagnostic error reports, though.

In detail, contrast chart reviews and experimental studies. Chart has
high external, low internal validity. Experimental is opposite.
Example of retro chart review: 21/100 Netherlands hospitals
participated.

Type  EV     IV
----  -----  -----
Chart high   low
Exper low    high

Tips for record review studies: inter-rater reliability was low. More
people didn't help the IRR. Because dx errors are rare, do power
calculations. If the record review questionnaire is too long or
unclear, or if incomplete training of reviewers, those are all
pitfalls. Important to predefine hypotheses and exclusion criteria (so
describe coincidental findings as exploratory.

Hindsight bias is hard to avoid, and it comes up when you look *only*
at the error cases[^zwaan2017]. Experimental study that replicated how
reviewers look at cases. Constructed so the dx process described was
the same, but the endings are different. People rate the process as
bad when the ending is bad. Also they don't agree hardly at all on the
cause (like representativeness, premature closure, etc.) of the error.

Availability bias study (recent or memorable)[^Schmidt2014]. Go read
these Wikipedia articles about Q fever or Legionnaire's disease. Then
later in the day sneak up and have them read a case. 55% correct if
exposed to wiki vs. 70% if not. Argued: this shows the bias exists,
but not how often it happens (or how often it helps for that matter).

Eye tracking. Kind of underused method. Also it's low intervention, doesn't
bother the subject. Eyelink1000 device. Chest xrays. Intervention is with the
patient description. So like have "cough" versus "fall" attached to a
clavicle fx xray. If you didn't see it in 2 seconds, you generally
didn't see it[^zwaanprep].

Further advice: not too many experimental conditions, beware
confounding, make the setup very standardized like written
instructions so the experimenter doesn't give subtle hints,
preregister the study (hypotheses and exclusion), publish negative
results if you can.

[^zwaanprep]: Zwaan *et al.*, manuscript in prep.

[^graber2012]: Graber, 2012, *BMJ QS.*

[^zwaan2017]: Zwaan. Is bias in the Eye of the beholder? *BMJ Q S*
2017, 26:104.

[^Schmidt2014]: Schmidt *et al.* 2014




Hardeep Singh
--------

Conceptual models all wrong, some are more useful. What's been useful
to us? **Missed opportunities** [^singh-jc]. Sociotechnical[^socio]:
need to account for all 8 dimensions: content, hwsw, ui, org policy,
workflow and comm, personnel, meas and mon, external rules and regs. A
lot of time they forget rules/reg. Third, Safer Dx framework breaks
down the dx proc into 5 dimensions: encounter, test perf and interp,
consult, folow up and track, and finally patient at the
center[^safer]. NAM has a conceptual model too of course, the blue
loop and red dot figure.

missed harm result
------ ---- ------
+      +
+      -
-      +
-      -

Walk thru types of data/methods. Mark Graber in *AJM* wrote about
overconf. That inspired Hardeep's study of Clinician cognition[^confid].
Difficult vs. easy vignetttes. Hard should go with less confidence,
and asking for more help. But it didn't bear out. Didn't tell them
there was such a thing as easy/hard. Not a prevalence study (reporter
made a big deal of only 60% of the easy cases were answered
correctly). 3 experts were the source of the easy/hard rating.

Opportunistic data[^gandhi][^serious][^root]. Advantages.
Disadvantages: hard to go from Root Cause Analy recs. Paper "problem
with RCA" in BMJQS. A lot of them say "do more training, do more
policy." Disadvantage: *can't generalize to frequency or population
level.* Malpractice claims don't correlate great with error. Because
not all claims are based on error.

Large (claims and/or EMR) datasets[^hyper][^stroke-ed]. Gotta validate
thru some other mechanism or else it's just hypoth generating. "Data
suggests...." Need to f/u with record review study.

Info from clinicians[^olafor]. They are forthcoming and will say they
made a dx error[^dx-peds].

Record reviews in the *non-selective* fashion[^bergl]. Consecutive
chart reviews give better idea of frequency. But it's a lot of work.
Can also do in *high risk cohort* like cancer[^endosc][^lung]. Good
agreement $\kappa = 0.69$. Missed opportunity in like 30%. Same in
peds cancer.[^bhise]

How do you review records in the first place? We have Revised Safer Dx
Instrument[^rsdi]. Another instrument Safer Dx Process Breakdown
Supplement helps you analyze the record further: what type of problem
was it?

Emerging methods. Also prospective surveillance. See Murphy's papers
in *Chest,* and mammo, and *BMJQS* "Electronic health
record-based...." Return visit triggers have low ish PPV, but these
have better.

Bench to bookshelf to bedside. Make your work visible. Prepare first
and talk to them. You and Twitter are not the only way to get it out
there. Public and policymakers have to know about your stuff.
Secondly, have clinical/operational partners. **Write conclusions and
abstract with these organizations in mind:** WHO, NAM, CDC, HHS, AMA,
AHRQ. **Who is the audience apart from other researchers?** Then get
the paper out to practicing docs, if you think the audience is
practicing docs. Incentivize the researcher to make work more
relevant, so like if WHO or NQF is impacted and/or cites your work,
etc. Furthermore, people will ask you for resources, so make one, with
hyperlinks[^compendium].

[^compendium]: A Compendium of Best Practices for IMproving Test result follow-up.

[^rsdi]: Singh. Recommendations for using the Revised.... *Diagnosis* 2019.

[^bhise]: Bhise. An electronic trigger based on care escalation. BMJQS 2018.

[^lung]: Characteristics and predictors. J clin onc.

[^endosc]: Singh. Missed opportunities to initiate endoscopic. Am J GAstro 2009

[^bergl]: Bergl PA. Frequency, risk factors, causes, and consequences.

[^olafor]: Olafor. Using voluntary reports. BMJ Emerg Med Journal. 33(4)

[^dx-peds]: Singh. Errors of diagnosis in pediatric.

[^hyper]: Alore EA. Diagnosis and management of primary hyperparathyroidism.
JAMA internal med.

[^stroke-ed]: Newman-Toker. Missed diagnosis of stroke.

[^gandhi]: Gandhi T "Missed and Delayed" Annals IM

[^serious]: Newman-Toker. "Serious Misdiagnosis-related harms"

[^root]: Giardina. "Root Cause Analysis reports Help Identify Common factors in
delayed diagnosis." 

[^confid]: Meyer *et al.* Physicians' Diagnostic Accuracy, Confidence,
and Resource Requests: A Vignette Study. *JAMA intern med* 2013

[^safer]: Singh Sittig. BMJ Q S 2015.

[^socio]: Sittig, Singh. *Qual Safe Health Care* 2010

[^singh-jc]: Singh, *Jt comm j qual patient saf* 2015.




Rob El Kareh: Large datasets
--------

Existing databases. NSQIP, SEER are clinical. HCUP, Medicare are
claims. Then there are some linked ones: Seer-Medicare,
OptumLabs[^exist]. Drawbacks: nonfatal things may not be in there,
especially subjective pt outcomes.

What types of study: SPADE[^spade]. First visit with sx and benign dx,
followed by 2nd visit with dangerous dx. Look forward and look
back. There is a simpler look-forward, so like if there's a high
calcium, what percent got a PTH test. Further, of those with
hyperpara, what percent got treatment surgery[^hyper]. Same: of those
with high Cr, who got repeat test, made a dx of CKD, and/or got renal
consult[^creat]. Assess dx proc[^breast-dx]. Find those with breast
CA, and look back to see what was the timing of the prior testing.
"Natural history of diagnosis" is not known for a lot of scenarios. Is
3 weeks good, or is 6 weeks? What does "standard of care" really mean,
and how do we find positive and negative outliers.

**AI and predictive modeling.** Ridiculous amount of
terminology[^bigdata]. Example of radiology, EEG for depression,
pathology, ophto, derm. But complex data[^sepsis]. Where is AI going?
More images, more complex data (home monitoring, EHR, phone).
Replicating our current care, if our current care is biased, does not
help anything. It's also not AI versus people, it's people with vs.
people without AI[^derm]. Even if we do direct to consumer, what does
patient do with "It is 7.5% chance of melanoma." But also EHR is
inaccurate, and in a biased way, not random, so therefore just piling
up more data does not help.

**Feedback on outcomes.** There are barriers[^feedback]. Fear of
impact on colleague or yourself. Don't want to jeopardize relationship
betw pt and oncologist. Want them to bring it up, knowing that it will
be treated fairly by your organization. Individual feedback. We don't
know outcome because of these handoffs. Open loop[^openloop] learning.
If you don't know the outcome, you tend to think 'Oh I did OK.' People
want fb but don't look. UCSF did deliberate follow up[^delib]. Can
also make day team message the night team. Can also do auto outcomes
reports (readmit, transfer to surg, ICU, code, death), not to say that
any of those are errors. Also triggers can prompt reviews in real
time. Also diagnostic dashboards. Next direction: Does feedback impact
outcomes? What channel do you use for feedback?

[^delib]: Narayana. J Grad Med Educ 2017

[^openloop]: Croskerry P. Acad emerg med 2000.

[^feedback]: Lipitz-Snyderman A. BMJQS 2017; 26:892.

[^derm]: Zabem GA. How should Artificial Intelligence Screen for Skin
cancer. JAMA Dermatol 2018.

[^sepsis]: Nemati, ... Buchman TG. An interpretable Machine Learning Model.
Crit Care Med 2018.

[^bigdata]: sastat.org.za/sasa2017/big-data-dictionary

[^breast-dx]: Using administrative data to estimate time to breast cancer
diagnosis. Eur J Cancer Care.

[^creat]: Kaiser Permanente Creatinine Safety Program. AJM.

[^spade]: Liberman. Symptom Disease Pair Analysis of Diagnostic error.
BMJQS. 2018.

[^exist]: Murphy. Working with Existing Dabases. *Clin Colon Rectal Surg*
2013.




Traber Giardina
--------

Why qualitative? Some vars can't easily be measured. To talk to people
who are marginalized and not usually included/heard. And more reasons.
Examples of qual study Qs. Pro: go in depth, be flexible (whole study
doesn't have to crash because interview guide isn't working and your
Qs don't resonate, because you can ask new questions in the interview
setting), be exploratory, build theory. But requirements: time in the
field, time analyzing data, be OK with no firm guidelines and
evolving/changing field. Con: low generalizability because *you* are
selecting your population (mention that in paper *a priori* so you
don't get dinged), and researcher bias (ways to minimize that, but the
way you view the world does affect your analysis).

Major qual approaches: ethnography (from anthropology, immerse
yourself and focus on culture), phenomenology (focus on experience,
get the essence, from multiple stories), grounded theory (trying to be
more systematic, tend to understand the proc and devel a theory about
it, often biggish 20+ sample size), narrative (collect story, look at
the arc of story and the characters, often small sample), case study
(even smaller samp, like study one single hospital).

Applied qual research: is what I (Traber) do now. There is a research
question but no hypoth. Select an approp method. Do analysis. Applied
methods include: content analysis, framework analysis, constant
comparative method, thematic analysis.

In an example[^focused], dx requires dialogue, needed data often
absent, and distractions and time pressure are frequent. Observation
has advantages: can see stuff that might not be brought to an
interview situation. Con: can rely on memory, so take field notes
then, or very shortly after.

Interviews can be: unstruct (conversational, often used in grounded
theory, you probe but no list of Qs), semi-struct (you have a list of
Qs but...), and structured (same Qs for every last person, kind of
like a survey, avoid missing data). Example of interview leaning
toward unstruct[^uns]. Or semi struct, if it didn't come up we
specifically ask[^semi]. An interview that went wrong[^wrong]. Very short
answers, because portal was not meaningful, lots of back and forth.
That can happen no matter how flexible you try to be. Interview can be
f2f or phone. Interviews can get at sensitive data, which a survey may
not get (with dx error, they tend to start surface level but in
interview they open up as rapport gets built). Obviously in interview
you can clarify stuff you don't understand. No option in a survey to
dig deeper into any response. You have to practice and pilot your
interviews. Analysis can take *so* long. And yes, the flexibility: if
your guide changes as your process goes on, then it gets hard to
analyze the fact that you didn't ask questions of the early subjects.

Focus groups. Look at interaction or consensus. More cost effective. 3
groups of 8 is a lot of people (even though 6 may be better group
size). Big talkers can be a thing. Group dynamics can be a thing
(putting docs, nurses, admins in a room can mean some defer).

Existing documents[^exi]. Like what are poeple saying in the comments
sections. Find the right rsch Q approp for the data source. Or when
people answer the open ended Qs in a survey[^exi-survey].

Several ways to to sampling for qualitative research. We've done
maximal variation (get diff types of folks). THere's also deviant
sampling (get outliers). Snowball (if the population is hard to
access, get one person, get them to introduce you to more).
Convenience (the least rigorous, least likely to yield useful info,
not recommended, usually want to make a conscious choice).

How to analyze. Read texts, get *meaning units*, put those in
*categories*, and then group those into *themes.* Finally look for
overarching *findings* or theory. We use ATLAS-TI software: highligh
texts, assign codes, pull quotes. Sometimes iteratively pull similar
codes and recode. Who does it: 1 reviewer, or 2? Expectation is
becoming 2. Kappa doesn't make so much sense for unstructured; rather
use consensus.

Assessing qualitative research. There are some checklists. But can be
hard with word limits. Your methods should be very clear. Especially
if you aren't using one of the "traditions," be very transparent.
Acknowledge your perspective (that wasn't there in the past) but now
reviewers want to know the researcher's background, to understdand
potential biases.

Addressing Bias: peer debriefing can help. Several ways to increase
rigor.

Mixed methods: Start with qual to understand, then quant to measure in
focused way. Start with survey and it doesn't make sense so do
interview. Main ways: convergent parallel design[^cpd] (like patient cohort
plus doc interview), explanatory sequential, exploratory sequential,
embedded. Con: *lots* of resources, complicated analysis because you
have to think of both study designs, hard to squeeze into one
paper[^best-prac].

QA: "is there anything else?" Use silence. Reflection and journaling
practice.

[^best-prac]: Best practices for mixed methods research in the health
sciences.

[^cpd]: Danforth KN. Follow up of abnormal estimated gfr. AJKD.

[^exi]: Learnign from patients' experiences related to diagnostic errors
is essential.

[^exi-survey]: Online public reactions to frequency of diagnostic errors

[^wrong]: Giardina. Patient perceptions of receiving test results.

[^semi]: Singh. Exploring situational awareness.

[^uns]: Giardina. THe patient portal and abnormal test results. Patient
Experience Journal.

[^focused]: Chopra V. Focused ethnography of diagnosis. J hosp Med 2018.




Global QA
--------

Is dx error rate going down, last few years? In Netherlands, yes, a
bit. But yeah it fluctuates, and it's subjective, maybe reviewers got
scared. And that was dx error as proportion of all medical errors. Dx
error highly prevalent in the *preventable harm* category. But not
easy to measure rate really. "Elephant of patient safety" paper.

Don't get discouraged about one or even 5 rejections. Try Jane
Biosemantics.




Introduction to Diagnostic Error Course (Nov 10, 1pm)
========

Intro to DE and SIDM. Mark Graber MD neph
--------

2008 DEM conf. 2011 SIDM. 2014 Diagnosis journal. 2015 IOM report.
2016 Coalition to improve diagnosis in medicine. SIDM helped get a
bill get passed to give money to AHRQ $2 million. Another bill drafted
$8m for dx error research. ImproveDx newsletter. "Top 10 Patient
Safety Concerns for Healthcare Organizations 2018," ECRI institute. Dx
error is #1. Rory Staunton, scraped knee, dx with viral
gastroenteritis, returned and dx with Strep sepsis.

Definition: *Failure to establish accurate timely explanation of
patient's health problem(s) or communicate that explanation to the
patient.*

But what is timely, and what is accurate? Sooner or later we'll have
guidelines like Sepsis should be diagnosed within *x* hours. "My
definition is totally useless to study diagnosis in real time" because
you need to wait and find out the right diagnosis using gold standard.

Incidence. Autopsy studies: 10 - 30% there is something that was
completely missed, and which is relevant to the cause of death. But
autopsy studies have some limits.

. . .

320 cognitive errors. Faulty data gathering 14%. Remainder are
synthesis failure. Context failures (not on graph but common). Or
another failure: premature closure or essentially not having a
differential. Opposite of optimizing. Herbert A. Simon, economist and
cognitive psychologist: satisficing[^simon]. Quit as soon as you find
a thing that meets some threshold, rather than going farther and
finding optimal.

[^simon]: Simon, Herbert A. (1956). "Rational Choice and the Structure
of the Environment." Psychological Review. 63 (2): 129-138.




Cognitive Failure & Clin Decision Making. Pat Croskerry MD PhD (psycholo)
--------

BMJ ~2017, 600k heart disease, 580k cancer, 250k medical error deaths.
41k suicide 30k firearms. Newman-Toker says diagnostic failure is the
biggest problem in patient safety. 40k-80k preventable hospital deaths
due to error[^leape]. Visual specialties not so big as IM, FM, ER.

Six clusters in dx failure: physician (intellect, knowledge, gender,
experience, age), cognitive (personality, openmindedness, persevering,
*rationality*, logicality, mindfulness, lateral thinking), decision
maker homeostasis (sleep, affective state, cognitive load),
environment (system design, ergonomics, team factors), disease itself
(symptoms, signs, onset, pathognomonicity, mimics), patient (family,
friends).

Of 347 legal actions from Canadian med practice assoc, >200 were
caused by diagnosis. Is it the system or the person? Mostly
person[^reducing]. Students are interested in knowing stuff; harder to
sell them on learning how they think. Knowledge deficits are uncommon
in dx failure; more often it's a thinking deficit.

Some people don't believe that biases are "a thing," but are rather an
artifact of the psychology laboratory. "Evolution wouldn't have
selected for irrationality." But evolution might select for
satisficing with respect to finding shelter, finding a mate, etc.

"Cognitive Bias Codex, 2016." Lots of them, but you probably only have
to know the top 10 or so.


n  bias
-- ------------------
17 anchoring
16 dx momentum
14 confirmation
13 unpacking failure
   search satisficing
   framing
   ascertainment
   psych out error

Where in the process does each occur. Anchoring and framing start
early. Etc. Cognitive and affective biases were the top source of
failure. Knowledge deficits were there but quite low.

Improve rationality[^stano]. Rationality quotient. Defined as absence
of bias? Have to collect enough info, seek various POVs, seek nuance,
etc. Dual process theory: if you recognize what's going on (pattern
processor), go into intuition process, otherwise go to analytical. We
even know where in brain they're located. System 1 is prefrontal
ventromedia, inferior temporal. Sys 2 is prefrontal and posterior
occiptical. Think of looking at zoster on the trunk and knowing
immediately. There are strategies for debiasing. Recongize the bias
and undo it. People are pessimistic that you can do this but it does
seem to work. Need a number of different strategies too. Does require
effort and habit. Forcing functions, encourage metacognition, try
thinking the opposite. No longer an option to ignore the thinking
process[^ethic]. Good environments make for good decision making.

Further learning: TACT program online at Dalhousie. https://medicine.dal.ca/departments/core-units/cpd/faculty-development/programs/TACT.html

Q: "What really matters is experience, take care of a lot of gout,
recognize it better." Versus debiasing strategies. A: Panglossians
versus meliorists. Humans must be doing it right, vs. we can make it
better. Sez it's not equipoise.

[^ethic]: Stark M, Fins JJ. The ethical imperative to think about thinking.
*Cambridge Quarterly of Healthcare Ethics* 2014.

[^stano]: Stanovich *et al.* 2012

[^reducing]: Graber. Reducing diagnostic error. Acad med 2002.

[^leape]: Leape Berwick Bates Jama 2002.




Gordy Schiff MD pcp Cook county, BWH
--------

Straw man of how to improve diagnosis: more lectures, more
subspecialization, more exposure to great diagnosticians, more
objective evidence, redundancy and cross checking, more check boxes in
your EMR, talking to patients rather than computer, parse cognitive
from system errors (sez you can't really do that), accountability
(metrics, get rid of bad docs).

Really: acknowledge the potential for errors. Think less aobut
brilliance. "Genius diagnosticians make great stories but they don't
make great healthcare."[^berwick]

Dx process failure $\cap$ delayed or missed dx $\cap$ adverse
outcomes. Linda McDougal, you never had cancer, we mixed up your
specimen. That's all 3 venn elements[^venn]. DEER taxonomy: Diagnosis
Error Evaluation and Research. Access to care, history, physical,
tests. About 20 checkboxes in total. Only 50% of ordered cscopes get
done. Scheduling, patient changes mind, order prep, doing prep, having
a ride home. Overlap: system probs contribute to cognitive[^overlap]. Can't
take a travel hx if you're time-pressured, etc. Do we need more
metrics[^elusive]? What we need is less fear or blame, so people can
question a diagnosis.

Situational awareness. HROs tend to have people worry/anticipate about
errors. "Preoccupation with possibility of failure.... Rehearse familiar
scenarios of failure"[^reason].

**Pitfalls.** Mistaking one for the other: think MI or dissection is a
kidney stone, thinking bipolar is depression. Ignoring test limits
(false negative mammogram). Atypical presenta. Tying a new sx into a
chronic dx. Overlooking drug or environment cause. 4 Rs: rare, rapid,
remediable, really bad. 4 Cs (serious if miss): continuing exposure,
contagious, chronic progressive, confusing. Closed loop vs open
loop[^openloop-schiff]. Overconfident vs overwhelmed. Robocall[^robo] shows
that some still had sx after going home, and of those, only a minority
came back to see a doc. EHR prevent errors. CYA = a canvas for your
assessment (write out your ddx, likelihoods, etiology, urgency, etc).
Thinking out loud. Contingencies should be in there.

[^robo]: Berner ES. Exploration of an automated approach.

[^openloop-schiff]: Schiff. *A J Med* 2008

[^reason]: Reason. *Human Error.*

[^elusive]: Schiff GD. The Elusive and illusive quest for diagnostic
safety metrics.

[^overlap]: Gupta et al. *Diagnosis* 2018.

[^venn]: Schiff. advances in patient safety. AHRQ 2005. Schiff and Leape,
*Acad Med* 2012.

[^berwick]: Don Berwick qtd. in Boston Globe, 2002-07-14




Ashley Meyer: HIT and (how it affects) Diagnosis
--------

Focus especially on EHR. Help or a hindrance. "Unaided clinicians
often make diagnostic errors.... HIT... potential to
enable..."[^potential]

In 2008 it was majority paper[^adoption]. Other ways to reduce dx
error[^can-electronic]. But problems especially: templates (constrains
thinking), alters communication (less f2f), alert fatigue, other
effects on cognition. More probs but not covered in depth: copy paste,
info overload, burnout, altered workflow.

**Templated notes**[^ebola]. Dallas, Texas. Nurse was using a template to
encourage flu shots. The additional stuff in that template turned out
to be useful. Blame went to the EHR itself. Templates make less face
to face, more assuming it will be read.

**Altered relationship**[^derogate]. Patients lose trust if it's use a
computer, versus consult with colleague. Or in another example, auto
release of test result, maybe without interp[^auto-release]. Abnormal
number but doctors do not leave comments about what to do, what that
means, whether it's a problem.

**Alert fatigue.** Which to turn on and turn off.

General effects on cognition[^altering]. EHR improves ease of
accessing (no chasing paper), but also accessing is burdensome.
Likewise, situational awareness worse vs better.

[^altering]: Holden RJ. 2009. Cognitive performance-altering effects of
electronic. *Cogn Tch Work* 13:11.

[^auto-release]: Giardina. Releasing test results directly.

[^derogate]: Shaffer VA. Why do patients derogate physicians who use a
computer-based. *Med Decis Making*

[^ebola]: Upadhyay DK. Ebola US patient zero. *Diagnosis* 2016.

[^can-electronic]: Schiff GD. Can electronic clinical documentation.
*Nejm* 2010.

[^adoption]: Henry J. Adoption of electronic health record systems.

[^potential]: El Kareh *et al.* Use of health information technology.




Medical Error - Human Condition - Danielle Ofri (Nov 11)
========

Rx pad "Rule out AI, Rule out RA," meaning adrenal insufficiency, by
the way. Plus "regular" visit stuff. Time to think. There is no
"later." Clinical decision making here would not work: refer to
endocrine, felt bad. Inpatient it's relatively easier, with a big
crew, and everyone knows stakes are higher, but most medicine is
outpatient. Anything that requires thinking, I'm sunk. There is no CPT
code for contemplation.

Example. Bilat tingling, more likely atypical carpal tunnel than
something like MS. Trial of OTC splints. Better in 2 weeks is good
enough for primary care. Second opinion from cardiology, ordered
electrodiagnostic, plus coronary workup and periph vascular workup.
EMG NCS negative. But splints helped. Both docs here trying to avoid
diagnostic error, taking different approaches.

AI and the EMR don't have to commit. We have to commit: some
preliminary diagnosis. It's a tall order, and an incredibly stressful
one. Is the explanation for fatigue something straightforward, or is
it something horrible.

People don't just make errors because they're stupid. They also make
errors because they're smart. We have a need for shortcuts. Focus on
error mitigation: rapid recognition. F# on a cello: you have to
listen. Q: How do you play in tune? Yo-Yo Ma: I don't. I hear the
mistakes faster and I fix them. Communication causes error, but
solution goes like telling people "Communicate better!" Rather, make
it about asthma with simulated patient, and slip in stuff about
communication, include real-world distractions. Likewise never label
your thing "Fixing diagnostic error." Make your thing about
schizophrenia, or adrenal insufficiency instead.

Missed intracranial bleed. Read "radiology fine," didn't look at
images. Someone else caught it. Told no one. Took years to talk in
public about it. Self doubt, and no idea how many errors made
downstream, that week. Simulation can get emotions going.

*(My aside. Humorous exaggerations, never reuse password and never
write down, bad powerpoints. Note that changing advice about passwords
parallels the experience of consuming health advice.)*

I"m going to get out the tea and crumpets, crank up the Chopin... and
I'm going to admit the patient to medicine for an inpatient workup.

Q: Anything we can do to get more comfortable with uncertainty? A:
Great art and great literature deals well with uncertainty. A good
poem does that: living in an uncertainty moment, enjoying that moment.
Reading good literature exercises parts of your brain that you don't
usually. Deals in metaphor, and patients deal in metaphor too.
Bellevue Literary Review *q.v.*

RAND study: doctor happiness correlates to LDL.

**Debate of sorts:** Knowledge gaps? We Taught them wrong and
misinformation gets propagated? (Newman-Toker) Not enough time? (Ofri)
Cognitive biases? (talks yesterday)

*(Less memorization? Vs De-skilling?)*




Physical Exam - Carla Pugh (Nov 11)
========

Technology Enabled Clinical Improvement Center (TECI).

ER "He's had RLQ pain, 3 CTs in 4 weeks, we can't figure out what's
going on with him." He looks more annoyed by the process than in page.
Healthy 40 ish guy. LLQ fine. RLQ also no tenderness to light touch,
light palp, deep palp. But below inguinal, that is where the pain is.
No appy, not even hernia: groin pull. 85% of failure to dx cases have
inadequate assessment (which includes phys exam as part of).

How did we get here? Two factors. **Tech trust** (patients trust the CT
more, residents trust the CT more).

*(CT bad. Ultrasound good? Imaging good. Imaging bad. AI good. AI bad.
Sparkly looking android. AI good. AI bad. Time helps. Tech helps. EMR
bad.)*

Force over time graph of thyroid exam, one sensor per finger. Senssors
can go on mannequin parts too. Assertion: "Thirty times a second is
faster than a human can do it." Physical exam registry. Pelvic.
Someone said "why do you have a sensor on the os; no one touches it."
Lost in translation: either things that teacher doesn't say, or we're
not good at observing. (7 N = 0.71 kgf = 1.6 lbf)

DRE. Students were really fast, and applied less pressure, spent most
of time in the center. Students say "we see what the clinicians do,"
but they see the back of the hand, so they do pronate/supinate not fan
the finger side to side.

Breast exam. starts out linear where more force is needed[^sensor],
but beyond a certain point it doesn't help. Low force tends to be
ineffective. Also piano fingers is 4x less likely than the circular
rubbing. Vertical movement, they are about half and half of skilled
and unskilled. There is the search mode and assessment mode. What
about rectangular vs circular search (circumference), vs radial. The
students look at faculty and they choose a pattern, based on probably
not lots of factors.

Things that humans do subconsciously when touching an object.
"Exploratory maneuvers." Texture, hardness, temp, wt, volume, global
shape, exact shape.

"I'm not trying to kill the art. I just want the art to be digital."

"By the time we leave the room, everything we said and did should be
in the computer."

[^sensor]: Sensor technology in assessments of clinical skill. *nejm*




Diagnose the Diagnosis - Jason Waechter MD, U Calgary
========

teachingmedicine.com

Dx errors 74k deaths per year. Commonly it's too narrow of dx focus,
then inadequate test, misinterp of tests, and it goes on. Source:
Coverys claims. H&P, ordering tests, interpreting tests are big
sources. Cogn errors are a big part[^cogn-prevalent]. Overlap with
system. Rare 3.4% knowledge being faulty.

*Working through case online. Twenty-four year old woman with chest
pain. Awakened from sleep, on OCPs, sharp, left side, smoker,
apprehensive, worse with deep breathing, worse with motion, no cough
or sputum or fever. I didn't do ECG or pregnancy test, but should
have. Dissection on ddx. D-dimer or not?*

*Audience comments.*

[^cogn-prevalent]: *Arch int med* 2005; 165:1493




Value Equation (Nov 12)
========

Newman-Toker
--------

*(joined 08:30)*

5% GDP in one study. In Schwartz *et al.* "secret shoppers," med
record data alone underestimates the costs. CT head overuse. NHAMACS
data, dizziness, 40% get CT, only 3% MRI. Waste of $1 billion per
year, on this one problem alone[^billion].

Equation: $V = Q / C$, relating value, quality, and cost. About 5 main
patterns of how to increase value; obvs best is increase qual *while*
decreasing cost. "Dangerous called benign" is false negative, and
should be moved to true positive, and that saves lives. False positive
$\to$ true negative, on the other hand, saves money, and there are
many more of these.

EEG for HA: yes, it should be stamped out, and it's wasteful, but it's
rare, and most isn't in the "never do this" category. less qual, more
cost is waste, obvs bad. more Qual, more dollars = safety oriented,
nervous nellie. Less Q less C = efficiency, crazy cowboy. But bouncing
back and forth doesn't do anything but trading off. Holy grail is more
Q less C. Adding the **H**ead **I**mp **N**ystag **T**est **S**kew
test will move the ROC curve. "Doing different and better." See the
4-quadrant figure[^quadrants].

[quadrants^]: Newman-Toker 2013

**Policy barriers to implementing this.** Organ system orientation of
NIH is not great if you misdiagnose heart condition as kidney or
something, neither NHLBI nor NIDDK cares. Second, fee for service is
toxic to diagnosis. There's a theory that "you can do whatever tests
you want." But really it encourages go-fast and thoughtlessness. But
P4P assumes the dx is correct. At risk payments (DRG bundle
capitation)....

- Invest in diagnostic research

- Measure diagnostic outcomes. So like SPADE[^spade2]. Lots of return
visits for dizzy as opposed to heart.

- Reorient payments. Get rid of FFS. P4P maybe but demand dx accuracy
first, and pay more for good dx acc. For atrisk models, change to use
symptom related groups as bundles. So like $1000 for a dizzy patient
in ED.

Total financial harms of dx error probably **exceed $100 billion per
year** in USA.

[^spade2]: Liberman BMJQS 2018

[^billion]: newman-toker BMJQS 2013




Mark McClellan, MD PhD, Duke
--------

Pathways to address dx errors: Increase funding (currently $7 million
per year). Increase measure development. *Few measures focus on
diagnostic accuracy or errors. Measures can be used to track and
identify errors, inform quality improvement activities, and
incorporate into payment models. Have proven difficult to identify
directly.*

Third, payment reform. Not gonna get rid of FFS right away.
Incremental help by paying more for cognitive services like complex
diagnostics. Also improve the (currently limited) payment for improved
diagnostic technologies that can improve accuracy. Alternative payment
models are a critical step, after all.

**Where are we in payment reform?** Trad ffs $\to$ p4p $\to$ limited
population-based $\to$ complete populated based. We are in category 2
(basically ffs linked to qual and value), and early cat 3 (alt pmt
model but still built on ffs architecture). Cat 3 and 4 are linked to
people not services. Level of an episode of care or to whole
person[^lan].

[^lan]: HCP LAN Framework (Health Care Payment Learning & Action
Network). Alternative payment model framework. 2017 MITRE.




Michelle Schreiber, MD (of CMS)
--------

20 years later, *To Err Is Human* is a leadership failure. We're still
waiting for a safer system. Not a failure because "not enough quality
measures." We need the *right ones.* And timely ones, not ones that
give you data 3 years out. Instantaneous. Relevant to patients. Less
burdensome to providers. Future of measures? I don't know. It's an
interesting time.

CMS committed to:

- Meaningful measures initiative

- Burden reduction

- Transparency (incl CMS MyHealthEData, download claims data)

- Alignment (e.g. fix measures w/ diff HTN cutoffs)

- Electronic measures. Only way we'll have timely feedback. FHIR based
and open APIs.

- Patient voice. PROs are kind of clunky right now.




AI in Diagnosis (Nov 12)
========

Michael Howell, MD MPH, Google Health
--------

Successes in image recognition, translation. Identify western tussock
moth. Google translate with camera, real time. *Makes new things
feasible*.

DL $\subset$ ML $\subset$ AI. You don't need ML to make things
*appear* smart. Today it's easier to program a computer to learn than
it is to hard-code it to be smart. Email spam: no longer hard code a
bunch of if/then rules. General AI (doesn't exist) vs. specific AI. **AI
is math, not magic.** Nothing ('cept maybe blockchain) is trendier than
AI. Bring your native skepticism to the table. We publish in *JAMA*
and *Nature Medicine* not because it's fast but because it's important
to get it right. "AI will figure out the best possible treatment for
every patient," well, bring your skepticism.

Need a bunch more ophthalmologists to do screening properly. Chance of
getting the same grade from diff ophthalmologists is 60%. That's
*messiness of the real world.* Whole separate paper about how to you
deal with that. Lung cancer screening prevents death, but only 4.4% of
eligibles get the screen, and in the NLST still 96% of positive s were
false. Conclusion: yeah, it makes new things feasible.




Gordon Schiff, MD, BWH
--------

tk




Christina Silcox, PhD, Duke
--------






Process Measures (Nov 12, 1pm)
========

Shantanu Agrawal, MD MPhil, NQF
--------

David Baker, MD MPH FACP, Joint Commission
--------

Mary Barton, MD MPP
--------
